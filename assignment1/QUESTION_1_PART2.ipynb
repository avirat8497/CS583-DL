{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "sUCX4We62n0X",
    "outputId": "4ac8e453-f083-4609-c59d-994e407cb7dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import io\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3jll0CN42yIR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "MwpATKx1239n",
    "outputId": "e5b2c6fb-59fa-4786-950a-907153b9f319"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.572193</td>\n",
       "      <td>0.257732</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.573840</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.372014</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>0.561341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571053</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.274448</td>\n",
       "      <td>0.264505</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.550642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.320158</td>\n",
       "      <td>0.700535</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.375427</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.646933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.609626</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.989655</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.558360</td>\n",
       "      <td>0.556314</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.857347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.807487</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.495781</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.444795</td>\n",
       "      <td>0.259386</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.325963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834211</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.789655</td>\n",
       "      <td>0.643460</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.466724</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.835949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.459916</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.338737</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.721826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.278656</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.457806</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.321672</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.725392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177866</td>\n",
       "      <td>0.433155</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.334471</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744737</td>\n",
       "      <td>0.120553</td>\n",
       "      <td>0.486631</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.592827</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.454259</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807895</td>\n",
       "      <td>0.280632</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.628692</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.381399</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.878745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813158</td>\n",
       "      <td>0.146245</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.440928</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.365931</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.561497</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.743224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.550802</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.706751</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.534799</td>\n",
       "      <td>0.621969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.804416</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.905136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.542194</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.736091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.860526</td>\n",
       "      <td>0.233202</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.419795</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.164032</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.413249</td>\n",
       "      <td>0.453925</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.607703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.167984</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.304124</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.457413</td>\n",
       "      <td>0.633106</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.466403</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.325939</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.175889</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.533123</td>\n",
       "      <td>0.372867</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.358060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>0.689840</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.436709</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.350927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.221344</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.309278</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.562069</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.403785</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.226109</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.864469</td>\n",
       "      <td>0.525678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.191126</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.258893</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.476341</td>\n",
       "      <td>0.196246</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.706960</td>\n",
       "      <td>0.393723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.203557</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.283505</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.328076</td>\n",
       "      <td>0.300341</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.654066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597368</td>\n",
       "      <td>0.193676</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.390295</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.227816</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.718260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.554852</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.425868</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.454351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786842</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.419831</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.291809</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602632</td>\n",
       "      <td>0.494071</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.609215</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.265335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.624506</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.148276</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.194009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.470356</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.144828</td>\n",
       "      <td>0.259494</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.158345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.381423</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.268139</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.496587</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.104850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.460568</td>\n",
       "      <td>0.788396</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.395722</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.312303</td>\n",
       "      <td>0.539249</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.879447</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.262069</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.359621</td>\n",
       "      <td>0.564846</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.318830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.667984</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.135531</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.317241</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.429387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871053</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.204641</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.183794</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.160338</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.893345</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.243937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.610672</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.198738</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.113553</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.498024</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.123028</td>\n",
       "      <td>0.392491</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.286733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.054852</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.366894</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.208274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507895</td>\n",
       "      <td>0.535573</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.167192</td>\n",
       "      <td>0.341297</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.399209</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.195584</td>\n",
       "      <td>0.708191</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.240371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.233438</td>\n",
       "      <td>0.455631</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636842</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.297432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471053</td>\n",
       "      <td>0.519763</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.067511</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.176656</td>\n",
       "      <td>0.766212</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.711230</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.196552</td>\n",
       "      <td>0.105485</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.356467</td>\n",
       "      <td>0.629693</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.802139</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.220820</td>\n",
       "      <td>0.616041</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.251070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307895</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.360068</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.165478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.735495</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.136947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823684</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.718430</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.161172</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.970356</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.056962</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.205047</td>\n",
       "      <td>0.547782</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.172161</td>\n",
       "      <td>0.329529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.086498</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.699605</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.210345</td>\n",
       "      <td>0.073840</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.397290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.400856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.664032</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.675768</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.201141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    0.0  0.842105  0.191700  0.572193  0.257732  0.619565  0.627586   \n",
       "1    0.0  0.571053  0.205534  0.417112  0.030928  0.326087  0.575862   \n",
       "2    0.0  0.560526  0.320158  0.700535  0.412371  0.336957  0.627586   \n",
       "3    0.0  0.878947  0.239130  0.609626  0.319588  0.467391  0.989655   \n",
       "4    0.0  0.581579  0.365613  0.807487  0.536082  0.521739  0.627586   \n",
       "5    0.0  0.834211  0.201581  0.582888  0.237113  0.456522  0.789655   \n",
       "6    0.0  0.884211  0.223320  0.582888  0.206186  0.282609  0.524138   \n",
       "7    0.0  0.797368  0.278656  0.668449  0.360825  0.554348  0.558621   \n",
       "8    0.0  1.000000  0.177866  0.433155  0.175258  0.293478  0.627586   \n",
       "9    0.0  0.744737  0.120553  0.486631  0.278351  0.304348  0.689655   \n",
       "10   0.0  0.807895  0.280632  0.502674  0.381443  0.380435  0.679310   \n",
       "11   0.0  0.813158  0.146245  0.513369  0.319588  0.271739  0.420690   \n",
       "12   0.0  0.715789  0.195652  0.561497  0.278351  0.206522  0.558621   \n",
       "13   0.0  0.978947  0.195652  0.550802  0.041237  0.228261  0.731034   \n",
       "14   0.0  0.881579  0.223320  0.545455  0.072165  0.347826  0.800000   \n",
       "15   0.0  0.684211  0.211462  0.716578  0.340206  0.456522  0.644828   \n",
       "16   0.0  0.860526  0.233202  0.727273  0.484536  0.543478  0.627586   \n",
       "17   0.0  0.736842  0.164032  0.673797  0.484536  0.489130  0.679310   \n",
       "18   0.0  0.831579  0.167984  0.598930  0.304124  0.413043  0.800000   \n",
       "19   0.0  0.686842  0.466403  0.641711  0.237113  0.500000  0.593103   \n",
       "20   0.0  0.797368  0.175889  0.491979  0.278351  0.608696  0.696552   \n",
       "21   0.0  0.500000  0.604743  0.689840  0.412371  0.347826  0.493103   \n",
       "22   0.0  0.705263  0.221344  0.534759  0.309278  0.336957  0.562069   \n",
       "23   0.0  0.478947  0.169960  0.620321  0.371134  0.271739  0.517241   \n",
       "24   0.0  0.650000  0.211462  0.668449  0.484536  0.282609  0.534483   \n",
       "25   0.0  0.531579  0.258893  0.994652  0.742268  0.586957  0.568966   \n",
       "26   0.0  0.621053  0.203557  0.673797  0.283505  0.250000  0.644828   \n",
       "27   0.0  0.597368  0.193676  0.417112  0.329897  0.260870  0.489655   \n",
       "28   0.0  0.747368  0.229249  0.770053  0.453608  0.402174  0.679310   \n",
       "29   0.0  0.786842  0.185771  0.454545  0.278351  0.282609  0.575862   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "148  1.0  0.602632  0.494071  0.545455  0.561856  0.239130  0.327586   \n",
       "149  1.0  0.539474  0.624506  0.534759  0.561856  0.467391  0.148276   \n",
       "150  1.0  0.650000  0.470356  0.673797  0.690722  0.576087  0.144828   \n",
       "151  1.0  0.463158  0.381423  0.598930  0.587629  0.456522  0.172414   \n",
       "152  1.0  0.547368  0.229249  0.743316  0.768041  0.500000  0.420690   \n",
       "153  1.0  0.578947  0.505929  0.491979  0.407216  0.304348  0.282759   \n",
       "154  1.0  0.407895  0.108696  0.395722  0.484536  0.358696  0.172414   \n",
       "155  1.0  0.563158  0.879447  0.513369  0.587629  0.250000  0.262069   \n",
       "156  1.0  0.739474  0.667984  0.545455  0.458763  0.206522  0.282759   \n",
       "157  1.0  0.373684  0.452569  0.684492  0.845361  0.293478  0.317241   \n",
       "158  1.0  0.871053  0.185771  0.716578  0.742268  0.304348  0.627586   \n",
       "159  1.0  0.644737  0.183794  0.684492  0.613402  0.206522  0.558621   \n",
       "160  1.0  0.350000  0.610672  0.545455  0.536082  0.195652  0.455172   \n",
       "161  1.0  0.700000  0.498024  0.631016  0.484536  0.402174  0.293103   \n",
       "162  1.0  0.478947  0.500000  0.652406  0.587629  0.391304  0.231034   \n",
       "163  1.0  0.507895  0.535573  0.529412  0.407216  0.391304  0.141379   \n",
       "164  1.0  0.723684  0.399209  0.502674  0.587629  0.217391  0.127586   \n",
       "165  1.0  0.710526  0.715415  0.481283  0.613402  0.195652  0.103448   \n",
       "166  1.0  0.636842  0.584980  0.663102  0.639175  0.445652  0.248276   \n",
       "167  1.0  0.471053  0.519763  0.502674  0.458763  0.195652  0.172414   \n",
       "168  1.0  0.671053  0.363636  0.711230  0.716495  0.380435  0.196552   \n",
       "169  1.0  0.623684  0.762846  0.802139  0.742268  0.456522  0.344828   \n",
       "170  1.0  0.307895  0.452569  0.513369  0.432990  0.282609  0.093103   \n",
       "171  1.0  0.457895  0.326087  0.491979  0.458763  0.173913  0.141379   \n",
       "172  1.0  0.823684  0.349802  0.598930  0.484536  0.228261  0.241379   \n",
       "173  1.0  0.705263  0.970356  0.582888  0.510309  0.271739  0.241379   \n",
       "174  1.0  0.623684  0.626482  0.598930  0.639175  0.347826  0.282759   \n",
       "175  1.0  0.589474  0.699605  0.481283  0.484536  0.543478  0.210345   \n",
       "176  1.0  0.563158  0.365613  0.540107  0.484536  0.543478  0.231034   \n",
       "177  1.0  0.815789  0.664032  0.737968  0.716495  0.282609  0.368966   \n",
       "\n",
       "           7         8         9         10        11        12        13  \n",
       "0    0.573840  0.283019  0.593060  0.372014  0.455285  0.970696  0.561341  \n",
       "1    0.510549  0.245283  0.274448  0.264505  0.463415  0.780220  0.550642  \n",
       "2    0.611814  0.320755  0.757098  0.375427  0.447154  0.695971  0.646933  \n",
       "3    0.664557  0.207547  0.558360  0.556314  0.308943  0.798535  0.857347  \n",
       "4    0.495781  0.490566  0.444795  0.259386  0.455285  0.608059  0.325963  \n",
       "5    0.643460  0.396226  0.492114  0.466724  0.463415  0.578755  0.835949  \n",
       "6    0.459916  0.320755  0.495268  0.338737  0.439024  0.846154  0.721826  \n",
       "7    0.457806  0.339623  0.264984  0.321672  0.471545  0.846154  0.725392  \n",
       "8    0.556962  0.301887  0.495268  0.334471  0.487805  0.578755  0.547076  \n",
       "9    0.592827  0.169811  0.454259  0.506826  0.430894  0.835165  0.547076  \n",
       "10   0.628692  0.169811  0.621451  0.381399  0.626016  0.695971  0.878745  \n",
       "11   0.440928  0.245283  0.365931  0.317406  0.560976  0.567766  0.714693  \n",
       "12   0.510549  0.301887  0.441640  0.368601  0.544715  0.597070  0.743224  \n",
       "13   0.706751  0.566038  0.757098  0.351536  0.626016  0.534799  0.621969  \n",
       "14   0.696203  0.301887  0.804416  0.530717  0.585366  0.633700  0.905136  \n",
       "15   0.542194  0.320755  0.331230  0.513652  0.650407  0.589744  0.736091  \n",
       "16   0.590717  0.377358  0.492114  0.419795  0.479675  0.505495  0.714693  \n",
       "17   0.645570  0.509434  0.413249  0.453925  0.528455  0.476190  0.607703  \n",
       "18   0.757384  0.358491  0.457413  0.633106  0.609756  0.567766  1.000000  \n",
       "19   0.567511  0.075472  0.394322  0.325939  0.390244  0.765568  0.404422  \n",
       "20   0.597046  0.207547  0.533123  0.372867  0.495935  0.893773  0.358060  \n",
       "21   0.436709  0.226415  0.495268  0.274744  0.447154  0.824176  0.350927  \n",
       "22   0.535865  0.264151  0.403785  0.215017  0.512195  1.000000  0.539943  \n",
       "23   0.428270  0.245283  0.331230  0.226109  0.495935  0.864469  0.525678  \n",
       "24   0.478903  0.283019  0.394322  0.191126  0.520325  0.934066  0.404422  \n",
       "25   0.493671  0.641509  0.476341  0.196246  0.528455  0.706960  0.393723  \n",
       "26   0.548523  0.396226  0.328076  0.300341  0.357724  0.714286  0.654066  \n",
       "27   0.390295  0.264151  0.296530  0.227816  0.439024  0.549451  0.718260  \n",
       "28   0.554852  0.452830  0.425868  0.274744  0.626016  0.780220  0.454351  \n",
       "29   0.419831  0.245283  0.495268  0.291809  0.455285  0.849817  0.539943  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "148  0.088608  0.603774  0.264984  0.609215  0.056911  0.128205  0.265335  \n",
       "149  0.221519  0.396226  0.230284  0.692833  0.073171  0.021978  0.194009  \n",
       "150  0.259494  0.169811  0.264984  0.624573  0.089431  0.010989  0.158345  \n",
       "151  0.215190  0.207547  0.268139  0.812287  0.000000  0.073260  0.144080  \n",
       "152  0.198312  0.245283  0.362776  0.496587  0.105691  0.021978  0.104850  \n",
       "153  0.103376  0.905660  0.460568  0.788396  0.065041  0.087912  0.283167  \n",
       "154  0.050633  0.754717  0.312303  0.539249  0.081301  0.102564  0.258203  \n",
       "155  0.061181  0.905660  0.359621  0.564846  0.097561  0.076923  0.318830  \n",
       "156  0.103376  0.660377  0.362776  0.659556  0.073171  0.135531  0.144080  \n",
       "157  0.050633  0.943396  0.230284  0.530717  0.154472  0.168498  0.429387  \n",
       "158  0.204641  0.754717  0.722397  1.000000  0.073171  0.252747  0.272468  \n",
       "159  0.160338  0.735849  0.593060  0.893345  0.073171  0.186813  0.243937  \n",
       "160  0.122363  0.698113  0.198738  0.543515  0.065041  0.113553  0.172611  \n",
       "161  0.046414  0.698113  0.123028  0.392491  0.390244  0.201465  0.286733  \n",
       "162  0.054852  0.886792  0.173502  0.366894  0.317073  0.307692  0.208274  \n",
       "163  0.075949  0.509434  0.167192  0.341297  0.162602  0.175824  0.283167  \n",
       "164  0.071730  0.528302  0.195584  0.708191  0.178862  0.150183  0.240371  \n",
       "165  0.027426  0.735849  0.233438  0.455631  0.243902  0.175824  0.172611  \n",
       "166  0.122363  0.566038  0.331230  0.802048  0.300813  0.106227  0.297432  \n",
       "167  0.067511  0.509434  0.176656  0.766212  0.195122  0.175824  0.290300  \n",
       "168  0.105485  0.490566  0.356467  0.629693  0.211382  0.194139  0.336662  \n",
       "169  0.130802  0.264151  0.220820  0.616041  0.154472  0.238095  0.251070  \n",
       "170  0.031646  0.509434  0.100946  0.360068  0.146341  0.205128  0.165478  \n",
       "171  0.035865  0.660377  0.072555  0.735495  0.073171  0.131868  0.136947  \n",
       "172  0.075949  0.584906  0.261830  0.718430  0.113821  0.161172  0.272468  \n",
       "173  0.056962  0.735849  0.205047  0.547782  0.130081  0.172161  0.329529  \n",
       "174  0.086498  0.566038  0.315457  0.513652  0.178862  0.106227  0.336662  \n",
       "175  0.073840  0.566038  0.296530  0.761092  0.089431  0.106227  0.397290  \n",
       "176  0.071730  0.754717  0.331230  0.684300  0.097561  0.128205  0.400856  \n",
       "177  0.088608  0.811321  0.296530  0.675768  0.105691  0.120879  0.201141  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data)\n",
    "data_normalized = pd.DataFrame(data_scaled)\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IEif6Mae27eU"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qXdWSfAZ2_MA"
   },
   "outputs": [],
   "source": [
    "X = data_normalized.iloc[:,1:14]\n",
    "y = data_normalized.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 20% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mMAzKrtZ3Chq"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.8, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nWDEPb5G5SQp"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HwfxRm-517k",
    "outputId": "cc314f1e-1dc0-46ad-9ce5-6c64f9ef31db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zI0Vtir54Iw",
    "outputId": "a11c4571-4334-4edc-ddee-3fa0c19a6ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm1 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 30% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L8Cit6TL57VH"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.7, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ad_Gb_fL6Sks"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_12DLiJY6UcO",
    "outputId": "5f00c9d4-a891-4b0a-9f9d-0104efb8aba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RY66uv846XRV",
    "outputId": "6d9e41f4-2e50-40b1-d722-377835aa57e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.98\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm2 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 40% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ToGRRghe6bq3"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.6, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-i9eROPr60A4",
    "outputId": "a4bc5c32-f778-418b-c999-54d46ab40f24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFnHtxfX61wb",
    "outputId": "4f1a28fa-a3e6-49e7-d680-9ae0d7280c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm3 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 50% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-mFvXtTw65W8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgl-r8EP7Apb",
    "outputId": "da0744eb-081a-4a5c-9170-da3654aa5bf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aonw0P2C7EA-",
    "outputId": "82f5d25a-2a98-4b4b-f5c4-ecbbfecacc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm4 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 60% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pcr-Rd007H4A"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tX6HifJI7XAN",
    "outputId": "6e0e4b3d-3f01-4d41-fe74-e18edbc1d653"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb_AGEIQ77HL",
    "outputId": "5adcde0b-ba64-4185-d977-f094e24d5204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm5 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 70% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pb6OSpdj7_lz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0uv_N1e8LD6",
    "outputId": "f8d347e3-0917-4b6d-86e0-ac93a19cf3b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQuSmheT8MzX",
    "outputId": "7ed3bfe2-2bb3-4a67-bec5-27652b5c7d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm6 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 80% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FOxMvmHq8QVY"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMl66jBX8aQA",
    "outputId": "89c4c36d-caa0-4542-9303-297ce52bcf52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6Zcy5gy8dSr",
    "outputId": "cb606675-932f-4728-9579-69235706f22c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm7 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 90% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5m0zru498gwm"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeE_fx_z8nLr",
    "outputId": "eb66d509-e0ef-4c21-815d-e7662157dd28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAzc-ypz8pSb",
    "outputId": "6690b05b-8327-428a-a582-9b85d285bc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm8 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 100% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MBnm1Cp-8tBw"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.0000001, random_state=1) \n",
    "#print(len(X_train))\n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zl0zW9-f81ad",
    "outputId": "586fc9a5-c2e4-4e0f-c32e-9d8f212f72f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIyKGK1l8525",
    "outputId": "412786ff-0680-4a6d-e1a2-9461a2fc3ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm9 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88ao05vr8-CN",
    "outputId": "20a3efb4-3cc3-48a1-857a-4e98a50730ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9473684210526315, 0.98, 0.9767441860465116, 0.9859154929577465, 0.9824561403508771, 0.9767441860465116, 0.9655172413793104, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "percentages = ['20','30','40','50','60','70','80','90','100']\n",
    "accuracy_results = [accuracy(cm1),accuracy(cm2),accuracy(cm3),accuracy(cm4),accuracy(cm5),accuracy(cm6),accuracy(cm7),accuracy(cm8),accuracy(cm9)]\n",
    "print(accuracy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of training percentages and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "k1twm2QU9h7d",
    "outputId": "bcd4b229-4033-46f9-b8f4-d53f8a83fbd1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(percentages,accuracy_results)\n",
    "plt.xlabel('Percentages of Training Dataset')\n",
    "plt.ylabel('Accuracy of the System')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s41jVqiy-Gtr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "QUESTION_1_PART2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
