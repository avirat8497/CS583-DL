{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "ytUQlzP-AO6m",
    "outputId": "8ab00274-725e-4a0f-cac0-51114f05618e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import io\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3r9w4fv9AYX9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "LnD9bSwMAi-7",
    "outputId": "4e6f0155-f0e0-4a5f-a31a-83b5c9f4e0bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.572193</td>\n",
       "      <td>0.257732</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.573840</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.372014</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>0.561341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571053</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.274448</td>\n",
       "      <td>0.264505</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.550642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.320158</td>\n",
       "      <td>0.700535</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.375427</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.646933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.609626</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.989655</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.558360</td>\n",
       "      <td>0.556314</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.857347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.807487</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.495781</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.444795</td>\n",
       "      <td>0.259386</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.325963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834211</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.789655</td>\n",
       "      <td>0.643460</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.466724</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.835949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.459916</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.338737</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.721826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.278656</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.457806</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.321672</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.725392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177866</td>\n",
       "      <td>0.433155</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.334471</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744737</td>\n",
       "      <td>0.120553</td>\n",
       "      <td>0.486631</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.592827</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.454259</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807895</td>\n",
       "      <td>0.280632</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.628692</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.381399</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.878745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813158</td>\n",
       "      <td>0.146245</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.440928</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.365931</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.561497</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.743224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.550802</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.706751</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.534799</td>\n",
       "      <td>0.621969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.804416</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.905136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.542194</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.736091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.860526</td>\n",
       "      <td>0.233202</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.419795</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.164032</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.413249</td>\n",
       "      <td>0.453925</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.607703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.167984</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.304124</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.457413</td>\n",
       "      <td>0.633106</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.466403</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.325939</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.175889</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.533123</td>\n",
       "      <td>0.372867</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.358060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>0.689840</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.436709</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.350927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.221344</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.309278</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.562069</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.403785</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.226109</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.864469</td>\n",
       "      <td>0.525678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.191126</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.258893</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.476341</td>\n",
       "      <td>0.196246</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.706960</td>\n",
       "      <td>0.393723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.203557</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.283505</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.328076</td>\n",
       "      <td>0.300341</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.654066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597368</td>\n",
       "      <td>0.193676</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.390295</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.227816</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.718260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.554852</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.425868</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.454351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786842</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.419831</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.291809</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602632</td>\n",
       "      <td>0.494071</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.609215</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.265335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.624506</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.148276</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.194009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.470356</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.144828</td>\n",
       "      <td>0.259494</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.158345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.381423</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.268139</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.496587</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.104850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.460568</td>\n",
       "      <td>0.788396</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.395722</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.312303</td>\n",
       "      <td>0.539249</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.879447</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.262069</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.359621</td>\n",
       "      <td>0.564846</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.318830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.667984</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.135531</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.317241</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.429387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871053</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.204641</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.183794</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.160338</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.893345</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.243937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.610672</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.198738</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.113553</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.498024</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.123028</td>\n",
       "      <td>0.392491</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.286733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.054852</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.366894</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.208274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507895</td>\n",
       "      <td>0.535573</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.167192</td>\n",
       "      <td>0.341297</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.399209</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.195584</td>\n",
       "      <td>0.708191</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.240371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.233438</td>\n",
       "      <td>0.455631</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636842</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.297432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471053</td>\n",
       "      <td>0.519763</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.067511</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.176656</td>\n",
       "      <td>0.766212</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.711230</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.196552</td>\n",
       "      <td>0.105485</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.356467</td>\n",
       "      <td>0.629693</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.802139</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.220820</td>\n",
       "      <td>0.616041</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.251070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307895</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.360068</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.165478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.735495</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.136947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823684</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.718430</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.161172</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.970356</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.056962</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.205047</td>\n",
       "      <td>0.547782</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.172161</td>\n",
       "      <td>0.329529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.086498</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.699605</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.210345</td>\n",
       "      <td>0.073840</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.397290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.400856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.664032</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.675768</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.201141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    0.0  0.842105  0.191700  0.572193  0.257732  0.619565  0.627586   \n",
       "1    0.0  0.571053  0.205534  0.417112  0.030928  0.326087  0.575862   \n",
       "2    0.0  0.560526  0.320158  0.700535  0.412371  0.336957  0.627586   \n",
       "3    0.0  0.878947  0.239130  0.609626  0.319588  0.467391  0.989655   \n",
       "4    0.0  0.581579  0.365613  0.807487  0.536082  0.521739  0.627586   \n",
       "5    0.0  0.834211  0.201581  0.582888  0.237113  0.456522  0.789655   \n",
       "6    0.0  0.884211  0.223320  0.582888  0.206186  0.282609  0.524138   \n",
       "7    0.0  0.797368  0.278656  0.668449  0.360825  0.554348  0.558621   \n",
       "8    0.0  1.000000  0.177866  0.433155  0.175258  0.293478  0.627586   \n",
       "9    0.0  0.744737  0.120553  0.486631  0.278351  0.304348  0.689655   \n",
       "10   0.0  0.807895  0.280632  0.502674  0.381443  0.380435  0.679310   \n",
       "11   0.0  0.813158  0.146245  0.513369  0.319588  0.271739  0.420690   \n",
       "12   0.0  0.715789  0.195652  0.561497  0.278351  0.206522  0.558621   \n",
       "13   0.0  0.978947  0.195652  0.550802  0.041237  0.228261  0.731034   \n",
       "14   0.0  0.881579  0.223320  0.545455  0.072165  0.347826  0.800000   \n",
       "15   0.0  0.684211  0.211462  0.716578  0.340206  0.456522  0.644828   \n",
       "16   0.0  0.860526  0.233202  0.727273  0.484536  0.543478  0.627586   \n",
       "17   0.0  0.736842  0.164032  0.673797  0.484536  0.489130  0.679310   \n",
       "18   0.0  0.831579  0.167984  0.598930  0.304124  0.413043  0.800000   \n",
       "19   0.0  0.686842  0.466403  0.641711  0.237113  0.500000  0.593103   \n",
       "20   0.0  0.797368  0.175889  0.491979  0.278351  0.608696  0.696552   \n",
       "21   0.0  0.500000  0.604743  0.689840  0.412371  0.347826  0.493103   \n",
       "22   0.0  0.705263  0.221344  0.534759  0.309278  0.336957  0.562069   \n",
       "23   0.0  0.478947  0.169960  0.620321  0.371134  0.271739  0.517241   \n",
       "24   0.0  0.650000  0.211462  0.668449  0.484536  0.282609  0.534483   \n",
       "25   0.0  0.531579  0.258893  0.994652  0.742268  0.586957  0.568966   \n",
       "26   0.0  0.621053  0.203557  0.673797  0.283505  0.250000  0.644828   \n",
       "27   0.0  0.597368  0.193676  0.417112  0.329897  0.260870  0.489655   \n",
       "28   0.0  0.747368  0.229249  0.770053  0.453608  0.402174  0.679310   \n",
       "29   0.0  0.786842  0.185771  0.454545  0.278351  0.282609  0.575862   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "148  1.0  0.602632  0.494071  0.545455  0.561856  0.239130  0.327586   \n",
       "149  1.0  0.539474  0.624506  0.534759  0.561856  0.467391  0.148276   \n",
       "150  1.0  0.650000  0.470356  0.673797  0.690722  0.576087  0.144828   \n",
       "151  1.0  0.463158  0.381423  0.598930  0.587629  0.456522  0.172414   \n",
       "152  1.0  0.547368  0.229249  0.743316  0.768041  0.500000  0.420690   \n",
       "153  1.0  0.578947  0.505929  0.491979  0.407216  0.304348  0.282759   \n",
       "154  1.0  0.407895  0.108696  0.395722  0.484536  0.358696  0.172414   \n",
       "155  1.0  0.563158  0.879447  0.513369  0.587629  0.250000  0.262069   \n",
       "156  1.0  0.739474  0.667984  0.545455  0.458763  0.206522  0.282759   \n",
       "157  1.0  0.373684  0.452569  0.684492  0.845361  0.293478  0.317241   \n",
       "158  1.0  0.871053  0.185771  0.716578  0.742268  0.304348  0.627586   \n",
       "159  1.0  0.644737  0.183794  0.684492  0.613402  0.206522  0.558621   \n",
       "160  1.0  0.350000  0.610672  0.545455  0.536082  0.195652  0.455172   \n",
       "161  1.0  0.700000  0.498024  0.631016  0.484536  0.402174  0.293103   \n",
       "162  1.0  0.478947  0.500000  0.652406  0.587629  0.391304  0.231034   \n",
       "163  1.0  0.507895  0.535573  0.529412  0.407216  0.391304  0.141379   \n",
       "164  1.0  0.723684  0.399209  0.502674  0.587629  0.217391  0.127586   \n",
       "165  1.0  0.710526  0.715415  0.481283  0.613402  0.195652  0.103448   \n",
       "166  1.0  0.636842  0.584980  0.663102  0.639175  0.445652  0.248276   \n",
       "167  1.0  0.471053  0.519763  0.502674  0.458763  0.195652  0.172414   \n",
       "168  1.0  0.671053  0.363636  0.711230  0.716495  0.380435  0.196552   \n",
       "169  1.0  0.623684  0.762846  0.802139  0.742268  0.456522  0.344828   \n",
       "170  1.0  0.307895  0.452569  0.513369  0.432990  0.282609  0.093103   \n",
       "171  1.0  0.457895  0.326087  0.491979  0.458763  0.173913  0.141379   \n",
       "172  1.0  0.823684  0.349802  0.598930  0.484536  0.228261  0.241379   \n",
       "173  1.0  0.705263  0.970356  0.582888  0.510309  0.271739  0.241379   \n",
       "174  1.0  0.623684  0.626482  0.598930  0.639175  0.347826  0.282759   \n",
       "175  1.0  0.589474  0.699605  0.481283  0.484536  0.543478  0.210345   \n",
       "176  1.0  0.563158  0.365613  0.540107  0.484536  0.543478  0.231034   \n",
       "177  1.0  0.815789  0.664032  0.737968  0.716495  0.282609  0.368966   \n",
       "\n",
       "           7         8         9         10        11        12        13  \n",
       "0    0.573840  0.283019  0.593060  0.372014  0.455285  0.970696  0.561341  \n",
       "1    0.510549  0.245283  0.274448  0.264505  0.463415  0.780220  0.550642  \n",
       "2    0.611814  0.320755  0.757098  0.375427  0.447154  0.695971  0.646933  \n",
       "3    0.664557  0.207547  0.558360  0.556314  0.308943  0.798535  0.857347  \n",
       "4    0.495781  0.490566  0.444795  0.259386  0.455285  0.608059  0.325963  \n",
       "5    0.643460  0.396226  0.492114  0.466724  0.463415  0.578755  0.835949  \n",
       "6    0.459916  0.320755  0.495268  0.338737  0.439024  0.846154  0.721826  \n",
       "7    0.457806  0.339623  0.264984  0.321672  0.471545  0.846154  0.725392  \n",
       "8    0.556962  0.301887  0.495268  0.334471  0.487805  0.578755  0.547076  \n",
       "9    0.592827  0.169811  0.454259  0.506826  0.430894  0.835165  0.547076  \n",
       "10   0.628692  0.169811  0.621451  0.381399  0.626016  0.695971  0.878745  \n",
       "11   0.440928  0.245283  0.365931  0.317406  0.560976  0.567766  0.714693  \n",
       "12   0.510549  0.301887  0.441640  0.368601  0.544715  0.597070  0.743224  \n",
       "13   0.706751  0.566038  0.757098  0.351536  0.626016  0.534799  0.621969  \n",
       "14   0.696203  0.301887  0.804416  0.530717  0.585366  0.633700  0.905136  \n",
       "15   0.542194  0.320755  0.331230  0.513652  0.650407  0.589744  0.736091  \n",
       "16   0.590717  0.377358  0.492114  0.419795  0.479675  0.505495  0.714693  \n",
       "17   0.645570  0.509434  0.413249  0.453925  0.528455  0.476190  0.607703  \n",
       "18   0.757384  0.358491  0.457413  0.633106  0.609756  0.567766  1.000000  \n",
       "19   0.567511  0.075472  0.394322  0.325939  0.390244  0.765568  0.404422  \n",
       "20   0.597046  0.207547  0.533123  0.372867  0.495935  0.893773  0.358060  \n",
       "21   0.436709  0.226415  0.495268  0.274744  0.447154  0.824176  0.350927  \n",
       "22   0.535865  0.264151  0.403785  0.215017  0.512195  1.000000  0.539943  \n",
       "23   0.428270  0.245283  0.331230  0.226109  0.495935  0.864469  0.525678  \n",
       "24   0.478903  0.283019  0.394322  0.191126  0.520325  0.934066  0.404422  \n",
       "25   0.493671  0.641509  0.476341  0.196246  0.528455  0.706960  0.393723  \n",
       "26   0.548523  0.396226  0.328076  0.300341  0.357724  0.714286  0.654066  \n",
       "27   0.390295  0.264151  0.296530  0.227816  0.439024  0.549451  0.718260  \n",
       "28   0.554852  0.452830  0.425868  0.274744  0.626016  0.780220  0.454351  \n",
       "29   0.419831  0.245283  0.495268  0.291809  0.455285  0.849817  0.539943  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "148  0.088608  0.603774  0.264984  0.609215  0.056911  0.128205  0.265335  \n",
       "149  0.221519  0.396226  0.230284  0.692833  0.073171  0.021978  0.194009  \n",
       "150  0.259494  0.169811  0.264984  0.624573  0.089431  0.010989  0.158345  \n",
       "151  0.215190  0.207547  0.268139  0.812287  0.000000  0.073260  0.144080  \n",
       "152  0.198312  0.245283  0.362776  0.496587  0.105691  0.021978  0.104850  \n",
       "153  0.103376  0.905660  0.460568  0.788396  0.065041  0.087912  0.283167  \n",
       "154  0.050633  0.754717  0.312303  0.539249  0.081301  0.102564  0.258203  \n",
       "155  0.061181  0.905660  0.359621  0.564846  0.097561  0.076923  0.318830  \n",
       "156  0.103376  0.660377  0.362776  0.659556  0.073171  0.135531  0.144080  \n",
       "157  0.050633  0.943396  0.230284  0.530717  0.154472  0.168498  0.429387  \n",
       "158  0.204641  0.754717  0.722397  1.000000  0.073171  0.252747  0.272468  \n",
       "159  0.160338  0.735849  0.593060  0.893345  0.073171  0.186813  0.243937  \n",
       "160  0.122363  0.698113  0.198738  0.543515  0.065041  0.113553  0.172611  \n",
       "161  0.046414  0.698113  0.123028  0.392491  0.390244  0.201465  0.286733  \n",
       "162  0.054852  0.886792  0.173502  0.366894  0.317073  0.307692  0.208274  \n",
       "163  0.075949  0.509434  0.167192  0.341297  0.162602  0.175824  0.283167  \n",
       "164  0.071730  0.528302  0.195584  0.708191  0.178862  0.150183  0.240371  \n",
       "165  0.027426  0.735849  0.233438  0.455631  0.243902  0.175824  0.172611  \n",
       "166  0.122363  0.566038  0.331230  0.802048  0.300813  0.106227  0.297432  \n",
       "167  0.067511  0.509434  0.176656  0.766212  0.195122  0.175824  0.290300  \n",
       "168  0.105485  0.490566  0.356467  0.629693  0.211382  0.194139  0.336662  \n",
       "169  0.130802  0.264151  0.220820  0.616041  0.154472  0.238095  0.251070  \n",
       "170  0.031646  0.509434  0.100946  0.360068  0.146341  0.205128  0.165478  \n",
       "171  0.035865  0.660377  0.072555  0.735495  0.073171  0.131868  0.136947  \n",
       "172  0.075949  0.584906  0.261830  0.718430  0.113821  0.161172  0.272468  \n",
       "173  0.056962  0.735849  0.205047  0.547782  0.130081  0.172161  0.329529  \n",
       "174  0.086498  0.566038  0.315457  0.513652  0.178862  0.106227  0.336662  \n",
       "175  0.073840  0.566038  0.296530  0.761092  0.089431  0.106227  0.397290  \n",
       "176  0.071730  0.754717  0.331230  0.684300  0.097561  0.128205  0.400856  \n",
       "177  0.088608  0.811321  0.296530  0.675768  0.105691  0.120879  0.201141  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data)\n",
    "data_normalized = pd.DataFrame(data_scaled)\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Elm7InD6Ar7r"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2J09lK41A2Qd"
   },
   "outputs": [],
   "source": [
    "X = data_normalized.iloc[:,1:14]\n",
    "y = data_normalized.iloc[:,:1]\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWDlHgbWBP0J",
    "outputId": "35170a9c-956d-4ae8-ee58-253cac97a253"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=3).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm1 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpHUEzXWCMMK",
    "outputId": "39fb5dc7-2656-49ae-9efd-c708411671a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=4).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm2 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxJ0zs6IC4fO",
    "outputId": "97db3a05-f63c-4a67-a80e-2960c4781565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=5).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm3 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKuEPk7DDD0T",
    "outputId": "9dd7c986-bf31-4363-9736-f3a1658494e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=7).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm4 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnIEoTT6Dgdl",
    "outputId": "ace0f5a7-69a3-4c8e-eaa5-245309f8c1ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=8).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm5 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzOoIKm6Djlb",
    "outputId": "052f4d5a-9e95-414f-b984-86da4d42d4c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9824561403508771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=9).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm6 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-Rcs1vLDqpY",
    "outputId": "5f9e65f8-e46e-4faf-f91a-3dfcad8f95a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=10).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm7 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hb4SB-PYDtaK",
    "outputId": "cafe0421-3495-4683-d3da-9fcdec919c7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=12).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm8 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zsf4_jmaDwWq",
    "outputId": "eb78866f-14bf-4c89-cfd4-e1770be721d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9824561403508771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=13).fit_transform(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=1)\n",
    "#print(len(X_test))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1) \n",
    "#print(len(X_train))\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "cm9 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExXBw79TDytL",
    "outputId": "b8e4e502-267f-4a5d-b495-5ad5a312b699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9473684210526315, 0.9473684210526315, 1.0, 0.9649122807017544, 0.9649122807017544, 0.9824561403508771, 0.9649122807017544, 0.9649122807017544, 0.9824561403508771]\n"
     ]
    }
   ],
   "source": [
    "percentages = ['20','30','40','50','60','70','80','90','100']\n",
    "accuracy_results = [accuracy(cm1),accuracy(cm2),accuracy(cm3),accuracy(cm4),accuracy(cm5),accuracy(cm6),accuracy(cm7),accuracy(cm8),accuracy(cm9)]\n",
    "print(accuracy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "YTSZJ_CIEnZq",
    "outputId": "630197c2-fe55-4d20-dd92-3ec94cda9989"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(percentages,accuracy_results)\n",
    "plt.xlabel('Percentages of Selected Features')\n",
    "plt.ylabel('Accuracy of the System')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzWSX4OcEv4B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question_1_Part3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
