{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "l_R7L5YbknTL",
    "outputId": "6682d1b4-33d6-410c-a2b0-99f924e65164"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from google.colab import files\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EgggYkxSNz4I"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Q-fyzsVXPbYS",
    "outputId": "cbcbbcfc-466b-4b1b-f342-7ef19ec2ecd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.572193</td>\n",
       "      <td>0.257732</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.573840</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.372014</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>0.561341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571053</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.274448</td>\n",
       "      <td>0.264505</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.550642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.320158</td>\n",
       "      <td>0.700535</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.375427</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.646933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.609626</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.989655</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.558360</td>\n",
       "      <td>0.556314</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.857347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.807487</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.495781</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.444795</td>\n",
       "      <td>0.259386</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.325963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834211</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.789655</td>\n",
       "      <td>0.643460</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.466724</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.835949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.459916</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.338737</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.721826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.278656</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.457806</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.321672</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.725392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177866</td>\n",
       "      <td>0.433155</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.334471</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744737</td>\n",
       "      <td>0.120553</td>\n",
       "      <td>0.486631</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.592827</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.454259</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807895</td>\n",
       "      <td>0.280632</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.628692</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.381399</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.878745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813158</td>\n",
       "      <td>0.146245</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.440928</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.365931</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.561497</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.510549</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.743224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.550802</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.706751</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.757098</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.534799</td>\n",
       "      <td>0.621969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.223320</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.804416</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.905136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.542194</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.736091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.860526</td>\n",
       "      <td>0.233202</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.419795</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.714693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.164032</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.413249</td>\n",
       "      <td>0.453925</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.607703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.167984</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.304124</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.457413</td>\n",
       "      <td>0.633106</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.466403</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.325939</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.175889</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.533123</td>\n",
       "      <td>0.372867</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.358060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>0.689840</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.436709</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.350927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.221344</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.309278</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.562069</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.403785</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.226109</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.864469</td>\n",
       "      <td>0.525678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.191126</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.404422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.258893</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.476341</td>\n",
       "      <td>0.196246</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.706960</td>\n",
       "      <td>0.393723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.203557</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.283505</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.328076</td>\n",
       "      <td>0.300341</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.654066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597368</td>\n",
       "      <td>0.193676</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.390295</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.227816</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.718260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.554852</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.425868</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.626016</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.454351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786842</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.419831</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.495268</td>\n",
       "      <td>0.291809</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.539943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602632</td>\n",
       "      <td>0.494071</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.609215</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.265335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.624506</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.148276</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.194009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.470356</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.144828</td>\n",
       "      <td>0.259494</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.264984</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.158345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.381423</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.268139</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.229249</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.496587</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.104850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.460568</td>\n",
       "      <td>0.788396</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.395722</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.312303</td>\n",
       "      <td>0.539249</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.879447</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.262069</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.359621</td>\n",
       "      <td>0.564846</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.318830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.667984</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.135531</td>\n",
       "      <td>0.144080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.317241</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.429387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871053</td>\n",
       "      <td>0.185771</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.204641</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.183794</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.160338</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.893345</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.243937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.610672</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.198738</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.113553</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.498024</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.123028</td>\n",
       "      <td>0.392491</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.286733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.054852</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.366894</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.208274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507895</td>\n",
       "      <td>0.535573</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.167192</td>\n",
       "      <td>0.341297</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.283167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.399209</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.195584</td>\n",
       "      <td>0.708191</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.240371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.233438</td>\n",
       "      <td>0.455631</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.172611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636842</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.297432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471053</td>\n",
       "      <td>0.519763</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.067511</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.176656</td>\n",
       "      <td>0.766212</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.711230</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.196552</td>\n",
       "      <td>0.105485</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.356467</td>\n",
       "      <td>0.629693</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.802139</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.220820</td>\n",
       "      <td>0.616041</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.251070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307895</td>\n",
       "      <td>0.452569</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.360068</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.165478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.458763</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.735495</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.136947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823684</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.261830</td>\n",
       "      <td>0.718430</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.161172</td>\n",
       "      <td>0.272468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.970356</td>\n",
       "      <td>0.582888</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.056962</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.205047</td>\n",
       "      <td>0.547782</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.172161</td>\n",
       "      <td>0.329529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.282759</td>\n",
       "      <td>0.086498</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.336662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.699605</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.210345</td>\n",
       "      <td>0.073840</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.397290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.365613</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.400856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.664032</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.675768</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.201141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    0.0  0.842105  0.191700  0.572193  0.257732  0.619565  0.627586   \n",
       "1    0.0  0.571053  0.205534  0.417112  0.030928  0.326087  0.575862   \n",
       "2    0.0  0.560526  0.320158  0.700535  0.412371  0.336957  0.627586   \n",
       "3    0.0  0.878947  0.239130  0.609626  0.319588  0.467391  0.989655   \n",
       "4    0.0  0.581579  0.365613  0.807487  0.536082  0.521739  0.627586   \n",
       "5    0.0  0.834211  0.201581  0.582888  0.237113  0.456522  0.789655   \n",
       "6    0.0  0.884211  0.223320  0.582888  0.206186  0.282609  0.524138   \n",
       "7    0.0  0.797368  0.278656  0.668449  0.360825  0.554348  0.558621   \n",
       "8    0.0  1.000000  0.177866  0.433155  0.175258  0.293478  0.627586   \n",
       "9    0.0  0.744737  0.120553  0.486631  0.278351  0.304348  0.689655   \n",
       "10   0.0  0.807895  0.280632  0.502674  0.381443  0.380435  0.679310   \n",
       "11   0.0  0.813158  0.146245  0.513369  0.319588  0.271739  0.420690   \n",
       "12   0.0  0.715789  0.195652  0.561497  0.278351  0.206522  0.558621   \n",
       "13   0.0  0.978947  0.195652  0.550802  0.041237  0.228261  0.731034   \n",
       "14   0.0  0.881579  0.223320  0.545455  0.072165  0.347826  0.800000   \n",
       "15   0.0  0.684211  0.211462  0.716578  0.340206  0.456522  0.644828   \n",
       "16   0.0  0.860526  0.233202  0.727273  0.484536  0.543478  0.627586   \n",
       "17   0.0  0.736842  0.164032  0.673797  0.484536  0.489130  0.679310   \n",
       "18   0.0  0.831579  0.167984  0.598930  0.304124  0.413043  0.800000   \n",
       "19   0.0  0.686842  0.466403  0.641711  0.237113  0.500000  0.593103   \n",
       "20   0.0  0.797368  0.175889  0.491979  0.278351  0.608696  0.696552   \n",
       "21   0.0  0.500000  0.604743  0.689840  0.412371  0.347826  0.493103   \n",
       "22   0.0  0.705263  0.221344  0.534759  0.309278  0.336957  0.562069   \n",
       "23   0.0  0.478947  0.169960  0.620321  0.371134  0.271739  0.517241   \n",
       "24   0.0  0.650000  0.211462  0.668449  0.484536  0.282609  0.534483   \n",
       "25   0.0  0.531579  0.258893  0.994652  0.742268  0.586957  0.568966   \n",
       "26   0.0  0.621053  0.203557  0.673797  0.283505  0.250000  0.644828   \n",
       "27   0.0  0.597368  0.193676  0.417112  0.329897  0.260870  0.489655   \n",
       "28   0.0  0.747368  0.229249  0.770053  0.453608  0.402174  0.679310   \n",
       "29   0.0  0.786842  0.185771  0.454545  0.278351  0.282609  0.575862   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "148  1.0  0.602632  0.494071  0.545455  0.561856  0.239130  0.327586   \n",
       "149  1.0  0.539474  0.624506  0.534759  0.561856  0.467391  0.148276   \n",
       "150  1.0  0.650000  0.470356  0.673797  0.690722  0.576087  0.144828   \n",
       "151  1.0  0.463158  0.381423  0.598930  0.587629  0.456522  0.172414   \n",
       "152  1.0  0.547368  0.229249  0.743316  0.768041  0.500000  0.420690   \n",
       "153  1.0  0.578947  0.505929  0.491979  0.407216  0.304348  0.282759   \n",
       "154  1.0  0.407895  0.108696  0.395722  0.484536  0.358696  0.172414   \n",
       "155  1.0  0.563158  0.879447  0.513369  0.587629  0.250000  0.262069   \n",
       "156  1.0  0.739474  0.667984  0.545455  0.458763  0.206522  0.282759   \n",
       "157  1.0  0.373684  0.452569  0.684492  0.845361  0.293478  0.317241   \n",
       "158  1.0  0.871053  0.185771  0.716578  0.742268  0.304348  0.627586   \n",
       "159  1.0  0.644737  0.183794  0.684492  0.613402  0.206522  0.558621   \n",
       "160  1.0  0.350000  0.610672  0.545455  0.536082  0.195652  0.455172   \n",
       "161  1.0  0.700000  0.498024  0.631016  0.484536  0.402174  0.293103   \n",
       "162  1.0  0.478947  0.500000  0.652406  0.587629  0.391304  0.231034   \n",
       "163  1.0  0.507895  0.535573  0.529412  0.407216  0.391304  0.141379   \n",
       "164  1.0  0.723684  0.399209  0.502674  0.587629  0.217391  0.127586   \n",
       "165  1.0  0.710526  0.715415  0.481283  0.613402  0.195652  0.103448   \n",
       "166  1.0  0.636842  0.584980  0.663102  0.639175  0.445652  0.248276   \n",
       "167  1.0  0.471053  0.519763  0.502674  0.458763  0.195652  0.172414   \n",
       "168  1.0  0.671053  0.363636  0.711230  0.716495  0.380435  0.196552   \n",
       "169  1.0  0.623684  0.762846  0.802139  0.742268  0.456522  0.344828   \n",
       "170  1.0  0.307895  0.452569  0.513369  0.432990  0.282609  0.093103   \n",
       "171  1.0  0.457895  0.326087  0.491979  0.458763  0.173913  0.141379   \n",
       "172  1.0  0.823684  0.349802  0.598930  0.484536  0.228261  0.241379   \n",
       "173  1.0  0.705263  0.970356  0.582888  0.510309  0.271739  0.241379   \n",
       "174  1.0  0.623684  0.626482  0.598930  0.639175  0.347826  0.282759   \n",
       "175  1.0  0.589474  0.699605  0.481283  0.484536  0.543478  0.210345   \n",
       "176  1.0  0.563158  0.365613  0.540107  0.484536  0.543478  0.231034   \n",
       "177  1.0  0.815789  0.664032  0.737968  0.716495  0.282609  0.368966   \n",
       "\n",
       "           7         8         9         10        11        12        13  \n",
       "0    0.573840  0.283019  0.593060  0.372014  0.455285  0.970696  0.561341  \n",
       "1    0.510549  0.245283  0.274448  0.264505  0.463415  0.780220  0.550642  \n",
       "2    0.611814  0.320755  0.757098  0.375427  0.447154  0.695971  0.646933  \n",
       "3    0.664557  0.207547  0.558360  0.556314  0.308943  0.798535  0.857347  \n",
       "4    0.495781  0.490566  0.444795  0.259386  0.455285  0.608059  0.325963  \n",
       "5    0.643460  0.396226  0.492114  0.466724  0.463415  0.578755  0.835949  \n",
       "6    0.459916  0.320755  0.495268  0.338737  0.439024  0.846154  0.721826  \n",
       "7    0.457806  0.339623  0.264984  0.321672  0.471545  0.846154  0.725392  \n",
       "8    0.556962  0.301887  0.495268  0.334471  0.487805  0.578755  0.547076  \n",
       "9    0.592827  0.169811  0.454259  0.506826  0.430894  0.835165  0.547076  \n",
       "10   0.628692  0.169811  0.621451  0.381399  0.626016  0.695971  0.878745  \n",
       "11   0.440928  0.245283  0.365931  0.317406  0.560976  0.567766  0.714693  \n",
       "12   0.510549  0.301887  0.441640  0.368601  0.544715  0.597070  0.743224  \n",
       "13   0.706751  0.566038  0.757098  0.351536  0.626016  0.534799  0.621969  \n",
       "14   0.696203  0.301887  0.804416  0.530717  0.585366  0.633700  0.905136  \n",
       "15   0.542194  0.320755  0.331230  0.513652  0.650407  0.589744  0.736091  \n",
       "16   0.590717  0.377358  0.492114  0.419795  0.479675  0.505495  0.714693  \n",
       "17   0.645570  0.509434  0.413249  0.453925  0.528455  0.476190  0.607703  \n",
       "18   0.757384  0.358491  0.457413  0.633106  0.609756  0.567766  1.000000  \n",
       "19   0.567511  0.075472  0.394322  0.325939  0.390244  0.765568  0.404422  \n",
       "20   0.597046  0.207547  0.533123  0.372867  0.495935  0.893773  0.358060  \n",
       "21   0.436709  0.226415  0.495268  0.274744  0.447154  0.824176  0.350927  \n",
       "22   0.535865  0.264151  0.403785  0.215017  0.512195  1.000000  0.539943  \n",
       "23   0.428270  0.245283  0.331230  0.226109  0.495935  0.864469  0.525678  \n",
       "24   0.478903  0.283019  0.394322  0.191126  0.520325  0.934066  0.404422  \n",
       "25   0.493671  0.641509  0.476341  0.196246  0.528455  0.706960  0.393723  \n",
       "26   0.548523  0.396226  0.328076  0.300341  0.357724  0.714286  0.654066  \n",
       "27   0.390295  0.264151  0.296530  0.227816  0.439024  0.549451  0.718260  \n",
       "28   0.554852  0.452830  0.425868  0.274744  0.626016  0.780220  0.454351  \n",
       "29   0.419831  0.245283  0.495268  0.291809  0.455285  0.849817  0.539943  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "148  0.088608  0.603774  0.264984  0.609215  0.056911  0.128205  0.265335  \n",
       "149  0.221519  0.396226  0.230284  0.692833  0.073171  0.021978  0.194009  \n",
       "150  0.259494  0.169811  0.264984  0.624573  0.089431  0.010989  0.158345  \n",
       "151  0.215190  0.207547  0.268139  0.812287  0.000000  0.073260  0.144080  \n",
       "152  0.198312  0.245283  0.362776  0.496587  0.105691  0.021978  0.104850  \n",
       "153  0.103376  0.905660  0.460568  0.788396  0.065041  0.087912  0.283167  \n",
       "154  0.050633  0.754717  0.312303  0.539249  0.081301  0.102564  0.258203  \n",
       "155  0.061181  0.905660  0.359621  0.564846  0.097561  0.076923  0.318830  \n",
       "156  0.103376  0.660377  0.362776  0.659556  0.073171  0.135531  0.144080  \n",
       "157  0.050633  0.943396  0.230284  0.530717  0.154472  0.168498  0.429387  \n",
       "158  0.204641  0.754717  0.722397  1.000000  0.073171  0.252747  0.272468  \n",
       "159  0.160338  0.735849  0.593060  0.893345  0.073171  0.186813  0.243937  \n",
       "160  0.122363  0.698113  0.198738  0.543515  0.065041  0.113553  0.172611  \n",
       "161  0.046414  0.698113  0.123028  0.392491  0.390244  0.201465  0.286733  \n",
       "162  0.054852  0.886792  0.173502  0.366894  0.317073  0.307692  0.208274  \n",
       "163  0.075949  0.509434  0.167192  0.341297  0.162602  0.175824  0.283167  \n",
       "164  0.071730  0.528302  0.195584  0.708191  0.178862  0.150183  0.240371  \n",
       "165  0.027426  0.735849  0.233438  0.455631  0.243902  0.175824  0.172611  \n",
       "166  0.122363  0.566038  0.331230  0.802048  0.300813  0.106227  0.297432  \n",
       "167  0.067511  0.509434  0.176656  0.766212  0.195122  0.175824  0.290300  \n",
       "168  0.105485  0.490566  0.356467  0.629693  0.211382  0.194139  0.336662  \n",
       "169  0.130802  0.264151  0.220820  0.616041  0.154472  0.238095  0.251070  \n",
       "170  0.031646  0.509434  0.100946  0.360068  0.146341  0.205128  0.165478  \n",
       "171  0.035865  0.660377  0.072555  0.735495  0.073171  0.131868  0.136947  \n",
       "172  0.075949  0.584906  0.261830  0.718430  0.113821  0.161172  0.272468  \n",
       "173  0.056962  0.735849  0.205047  0.547782  0.130081  0.172161  0.329529  \n",
       "174  0.086498  0.566038  0.315457  0.513652  0.178862  0.106227  0.336662  \n",
       "175  0.073840  0.566038  0.296530  0.761092  0.089431  0.106227  0.397290  \n",
       "176  0.071730  0.754717  0.331230  0.684300  0.097561  0.128205  0.400856  \n",
       "177  0.088608  0.811321  0.296530  0.675768  0.105691  0.120879  0.201141  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data)\n",
    "data_normalized = pd.DataFrame(data_scaled)\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mfsXEM_jn-9G"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j8u6IfqSO9nj"
   },
   "outputs": [],
   "source": [
    "X = data_normalized.iloc[:,1:14]\n",
    "#print(X)\n",
    "y = data_normalized.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "keOqwUClOuTw"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) \n",
    "y_train=y_train.astype('int')\n",
    "y_val=y_val.astype('int')\n",
    "#print(y_train)\n",
    "#print(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHxSdrImt7j_"
   },
   "source": [
    "## Learning rate = 0.00001 and Batch Size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oYuZjITaRHfN"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.00001,batch_size= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DdfgQBWmXFL",
    "outputId": "ca8156ac-02c1-4437-dbc5-8a212649cbb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:351: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=150, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=1e-05, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DxGg3VHumfDf"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o0o8iaWVnO3m"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwmMPv4znXb9",
    "outputId": "5f97a727-33b8-4d6e-df3d-c21a026cb005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikklv0vhuJ16"
   },
   "source": [
    "## Learning rate = 0.001 and batch size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XI9H5UNRuZm8"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bzTYLiov5zI",
    "outputId": "dd1afcfa-eb1e-46c2-a086-26e2df0c33da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4k73jzN8v8zz",
    "outputId": "a7ba9b4e-0f4d-4b88-ec00-718be2f9ef66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm2 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIKwrox6wG90"
   },
   "source": [
    "## Learning rate = 0.0001 and batch size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aU0cooSWudG2"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.0001,batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeuCNvXqvJkP",
    "outputId": "ba4ca8ba-31ea-46b4-9683-bdd8f7546a99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7BdpmHhwbzS",
    "outputId": "21f458ae-26eb-46db-bde3-162ed47da663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm3 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fihFO1r8w9dz"
   },
   "source": [
    "## Learning rate = 0.0001 and batch size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QndYlFw2xDB0"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.0001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZApN9_-BxHRW",
    "outputId": "88436986-def0-469a-a842-19c67267f21b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqXyGrskxJUH",
    "outputId": "a82a430b-2db3-476f-f12a-c31db506a224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm4 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzQfW4zBxThR"
   },
   "source": [
    "## Learning rate = 0.001 and batch size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JxHLK1gXxPDT"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100), max_iter= 100,activation = 'relu',solver='sgd',random_state=1,learning_rate_init= 0.001,batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBGUNrisxh8l",
    "outputId": "f694a7e6-aea3-4cab-ef60-72ea5e56bc4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(150, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5GtGghtxlsb",
    "outputId": "fad05ff6-7d3b-4a5f-c378-288b8978af38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "cm5 = confusion_matrix(y_pred, y_val)\n",
    "print(\"Accuracy of MLPClassifier : \", accuracy(cm5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZwyR_PMxpZa",
    "outputId": "757730bd-0d40-4972-eb83-5ac3613fa74d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2222222222222222,\n",
       " 0.7777777777777778,\n",
       " 0.8611111111111112,\n",
       " 0.7777777777777778,\n",
       " 0.9722222222222222]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_results = [accuracy(cm1),accuracy(cm2),accuracy(cm3),accuracy(cm4),accuracy(cm5)]\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fyfCt4o9ymZV"
   },
   "outputs": [],
   "source": [
    "parameter_values = ['par1','par2','par3','par4','par5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of Parameter values vs accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "4cpUKFQRznxX",
    "outputId": "ef5068e1-88fe-40fd-d9c2-34524240cf3a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(parameter_values,accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "LbfN7Kx6zzsi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework1_QUESTION1_PART1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
